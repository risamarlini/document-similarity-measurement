{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "praktikum 1 nama hebry yanisa manihuruk i 12s16001 tugas nomor 1 s breadth first search s depth first search s uniform cost search tugas nomor 2 s breadth first search s depth first search\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import tokenize\n",
    "import unidecode\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# text1 = \"Heloo ! ThIs's  ã sent tokenize test  wiith misstkaes in speling.  this's sent two. is this sent three? sent 4 is cool! Now it's your turn.\"\n",
    "with open('papers/Laporan_12S16001.txt', 'r') as myfile:\n",
    "    data=myfile.read().replace('\\n', '')\n",
    "\n",
    "def remove_accent_before_tokens(sentences):\n",
    "    res = unidecode.unidecode(sentences)\n",
    "    return(res)\n",
    "    tmp = remove_accent_before_tokens(data)\n",
    "\n",
    "\"\"\"\n",
    "Removing accent marks and other diacritics - After tokens words\n",
    "\"\"\"\n",
    "\n",
    "def remove_accent_after_tokens(tokens):\n",
    "    tokens = [unidecode.unidecode(token) for token in tokens]\n",
    "    return(tokens)\n",
    "    tokens = remove_accent_after_tokens(tokens)\n",
    "    \n",
    "def remove_before_token(sentence, keep_apostrophe = False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophe:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]'\n",
    "        filtered_sentence = re.sub(PATTERN, r' ', sentence)\n",
    "    else :\n",
    "        PATTERN = r'[^a-zA-Z0-9]'\n",
    "        filtered_sentence = re.sub(PATTERN, r' ', sentence)\n",
    "    return(filtered_sentence)\n",
    "    remove_before_token(data)\n",
    "\n",
    "\"\"\"\n",
    "After tokenization  \n",
    "\"\"\"\n",
    "def remove_after_token(tokens): \n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation))) \n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens]) \n",
    "    filtered_text = ' '.join(filtered_tokens) \n",
    "    return filtered_text \n",
    "    remove_after_token(tokens)\n",
    "    \n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "WORDS = Counter(data)\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "    \n",
    "def correct_word_in_sentence(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    r = [correction(token) for token in tokens]\n",
    "    return (r)\n",
    "    tmp = ' '.join(correct_word_in_sentence(data))\n",
    "    type(tmp)\n",
    "    \n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "def removeStopwords_before_tokens(text, stopwords):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return [w for w in tokens if w not in stopwords]\n",
    "\n",
    "def preprocessing(content):\n",
    "    sentences = tokenize.sent_tokenize(content)\n",
    "    cleaned_sentences = []\n",
    "    for s in sentences :\n",
    "        # 1- Lower case \n",
    "        s = s.lower()\n",
    "        \n",
    "        # 2- Supprimer les blancs :\n",
    "        s = s.strip()  \n",
    "\n",
    "        # 3- Unicode - supprimer les accents\n",
    "        s = remove_accent_before_tokens(s)\n",
    "        \n",
    "        # 4- Remove punctuation ?\n",
    "        # A faire après avoir fait l'expanding contraction car sinon supprimer les ' qui symbolise la contraction\n",
    "        s = remove_before_token(s)\n",
    "\n",
    "        # 5- Correcting words\n",
    "        # s = ' '.join(correct_word_in_sentence(s))\n",
    "        s = ' '.join(correct_word_in_sentence(s))\n",
    "        \n",
    "        # 6- Remove StopWords\n",
    "        s = ' '.join(removeStopwords_before_tokens(s, stopwords))\n",
    "        \n",
    "        # Enregistrement des resultats\n",
    "        cleaned_sentences.append(s)\n",
    "    return(cleaned_sentences) \n",
    "        \n",
    "\n",
    "result = ' '.join(preprocessing(data))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "# For the example here, we are assuming space as an unique character, \n",
    "# Also capital letters are different than lower cased letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ex. \"I am Sam\" = { ['I '], [' a'], ['am'], ['m '], [' S'], ['Sa'], ['am']}\n",
    "def twoCharGram(dataset):\n",
    "    with open(dataset, 'r') as data:\n",
    "        textFile = data.read().replace('\\n', ' ')\n",
    "        kGrams = set()\n",
    "        # 2-Char gram\n",
    "        for i in range(len(textFile)-1):\n",
    "            if textFile[i] + textFile[i+1] not in kGrams:\n",
    "                kGrams.add(textFile[i] + textFile[i+1])\n",
    "    return kGrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jaccard Similarity is defined\n",
    "$JS(A,B)=$\n",
    "> # $\\frac{|A\\cap B|}{|A\\cap B|+ |A\\Delta B|} = \\frac{|A\\cap B|}{|A\\cup B|}$\n",
    "\n",
    "Consider two sets A = {0,1,2,5,6} and B = {0,2,3,5,7,9}. How similar are A and B? The Jaccard similarity is defined\n",
    "\n",
    "$JS(A, B) = \\frac{|A \\cap B|}{|A\\cup B|}$\n",
    "$= \\frac{|{0,2,5}|}{|{0,1,2,3,5,6,7,9}|} =  \\frac{3}{8} = 0.375$\n",
    "\n",
    "Given a set A, the cardinality of A denoted |A| counts how many elements are in A. The intersection between two sets A and B is denoted A ∩ B and reveals all items which are in both sets. The union between two sets A and B is denoted A ∪ B and reveals all items which are in either set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(D1, D2):\n",
    "    jaccard_calculation = 100.* len(D1.intersection(D2))/ len(D1.union(D2))\n",
    "    return jaccard_calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.32515337423312"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1set = twoCharGram(\"documents/Laporan_12S16001.txt\")\n",
    "D2set = twoCharGram(\"documents/Laporan_12S16002.txt\")\n",
    "D3set = twoCharGram(\"documents/laporan_12S16003.txt\")\n",
    "D4set = twoCharGram(\"documents/Laporan_12S16007.txt\")\n",
    "jaccard(D3set,D4set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Laporan1_12S16016.txt', 'Laporan_12S16001.txt', 'Laporan_12S16002.txt', 'laporan_12S16003.txt', 'Laporan_12S16007.txt', 'Laporan_12S16008.txt', 'laporan_12S16009.txt', 'Laporan_12S16010.txt', 'laporan_12S16014.txt', 'laporan_12s16018.txt', 'Laporan_12S16020.txt', 'Laporan_12s16021.txt', 'laporan_12S16022.txt', 'laporan_12S16024.txt', 'Laporan_12S16027.txt', 'laporan_12S16028.txt', 'laporan_12S16031.txt', 'laporan_12S16032.txt', 'laporan_12S16036.txt', 'Laporan_12S16037.txt', 'laporan_12S16038.txt', 'Laporan_12S16043.txt', 'laporan_12S16044.txt', 'Laporan_12S16046.txt', 'laporan_12S16047.txt', 'laporan_12S16048.txt', 'laporan_12s16049.txt', 'laporan_12S16050.txt', 'laporan_12S16051.txt', 'Laporan_12S16052.txt', 'laporan_12S16053.txt', 'laporan_12S16054.txt', 'laporan_12S16056.txt', 'laporan_12S16059.txt', 'laporan_12S16060.txt', 'Laporan_praktikum_sartika_12s16004.txt', 'Praktikum 1.txt', 'Praktikum1_12S16005.txt', 'praktikum1_12s16012.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andareas\\Anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py:570: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is saved\n"
     ]
    }
   ],
   "source": [
    "import gensim.models as gsm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import OrderedDict\n",
    "\n",
    "#path to the input corpus files\n",
    "train_corpus=\"papers\"\n",
    "\n",
    "#tagging the text files\n",
    "class DocIterator(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "            yield TaggedDocument(words=doc.split(), tags=[self.labels_list[idx]])\n",
    "\n",
    "docLabels = [f for f in listdir(train_corpus) if f.endswith('.txt')]\n",
    "print(docLabels)\n",
    "data = []\n",
    "for doc in docLabels:\n",
    "    data.append(open(join(train_corpus, doc), 'r', encoding='ISO-8859-1').read())\n",
    "    \n",
    "it = DocIterator(data, docLabels)\n",
    "\n",
    "#train doc2vec model\n",
    "model = gsm.Doc2Vec(size=300, window=10, min_count=1, workers=11,alpha=0.025, min_alpha=0.025) # use fixed learning rate\n",
    "model.build_vocab(it)\n",
    "model.train(it, total_examples=len(doc), epochs=20)\n",
    "\n",
    "\n",
    "model.save(\"training.model\")\n",
    "\n",
    "print(\"model is saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is loaded\n"
     ]
    }
   ],
   "source": [
    "#load the model\n",
    "model=\"training.model\"\n",
    "m=gsm.Doc2Vec.load(model)\n",
    "print(\"model is loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andareas\\Anaconda3\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Laporan_12S16020.txt', 0.4612135887145996),\n",
       " ('laporan_12s16049.txt', 0.2778031826019287),\n",
       " ('laporan_12S16009.txt', 0.2008456438779831),\n",
       " ('laporan_12S16048.txt', 0.18843130767345428),\n",
       " ('Laporan_12S16027.txt', 0.16920194029808044),\n",
       " ('Laporan_12S16001.txt', 0.1647045612335205),\n",
       " ('laporan_12S16056.txt', 0.13929666578769684),\n",
       " ('Praktikum1_12S16005.txt', 0.11301266402006149),\n",
       " ('Laporan1_12S16016.txt', 0.07793117314577103),\n",
       " ('praktikum1_12s16012.txt', 0.0754927322268486)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of test file\n",
    "test=\"test/Praktikum1_12S16015.txt\"\n",
    "new_test = open(join(test), 'r').read().split()\n",
    "inferred_docvec = m.infer_vector(test)\n",
    "m.docvecs.most_similar([inferred_docvec], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
